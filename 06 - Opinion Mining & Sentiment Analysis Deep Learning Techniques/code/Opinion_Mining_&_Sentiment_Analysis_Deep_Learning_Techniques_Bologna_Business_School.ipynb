{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Opinion_Mining_&_Sentiment_Analysis_Deep_Learning_Techniques_Bologna_Business_School.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "phOqAYXkijh9"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5-GCnvKijfl"
      },
      "source": [
        "# Opinion Mining & Sentiment Analysis: Deep Learning Techniques\n",
        "\n",
        "**Text Mining unit**\n",
        "\n",
        "_Prof. Gianluca Moro^, Dott. Ing. Nicola Piscaglia° – DISI, University of Bologna_\n",
        "\n",
        "^name.surname@unibo.it\n",
        "\n",
        "\n",
        "°name.surname@bbs.unibo.it\n",
        "\n",
        "\n",
        "**Bologna Business School** - Alma Mater Studiorum Università di Bologna"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdU4Z5sWijfn"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Import external libraries (thus verifying they are correctly installed)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luAnTvuXijfp"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "import gzip\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gensim\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbc8oQnRun1S"
      },
      "source": [
        "Check GPU and limit memory usage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jGBfE49k62l"
      },
      "source": [
        "devices = tf.config.experimental_list_devices()\n",
        "\n",
        "[print(device) for device in devices] # print all devices\n",
        "\n",
        "#!nvidia-smi # check GPU configuration\n",
        "\n",
        "if devices:\n",
        "  try:\n",
        "    tf.config.experimental.set_memory_growth = True\n",
        "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
        "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
        "  except RuntimeError as e:\n",
        "    # Virtual devices must be set before GPUs have been initialized\n",
        "    print(e)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRGJ_Ufdijf0"
      },
      "source": [
        "Define a utility function to download data files if they are not already present in working directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMRsk6PDijf1"
      },
      "source": [
        "import os\n",
        "from urllib.request import urlretrieve\n",
        "def download(file, url):\n",
        "    if not os.path.isfile(file):\n",
        "        urlretrieve(url, file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoSfmx3cijf7"
      },
      "source": [
        "## Deep Learning and Neural Networks for Text Mining and Sentiment Analysis\n",
        "\n",
        "_Deep learning_ denotes a general approach to machine learning employing **multi-layered models** to obtain **accurate representation of input data**: features are no longer extracted manually, but infered in the learning process\n",
        "\n",
        "DL is mostly based on _neural networks_, very flexible learning models with arbitrary complexity\n",
        "\n",
        "The use of DL and NN allows deep understanding of text without manually defining rules, lexicons, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22jD9LW4ijf9"
      },
      "source": [
        "## TensorFlow and Keras\n",
        "\n",
        "- **TensorFlow** by Google is one of the most used computation frameworks for deep learning\n",
        "  - TF works by building a _computational graph_ where each node represents an operation between _tensors_ (N-dimensional arrays)\n",
        "    - sums, products, derivatives, ...\n",
        "  - a graph can run either on CPU or (where available) on GPU for accelerated parallel computation\n",
        "- **Keras** provides an high-level API for building and training neural networks using TensorFlow as a backend\n",
        "  - networks can be built simply by stacking different layers with many configurable (hyper)parameters\n",
        "  - high-level commands are provided to train and evaluate networks on given datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MldDiptijf-"
      },
      "source": [
        "## Dataset: Movie Reviews\n",
        "\n",
        "We have a collection of user reviews extracted from IMDb (the _Internet Movie Database_) labeled as positive or negative\n",
        "\n",
        "We want to train a model to understand the positive or negative orientation of any review\n",
        "\n",
        "We start by loading the training dataset, containing 25,000 samples with two attributes\n",
        "- `label` indicates the orientation of the review, can be \"pos\" or \"neg\"\n",
        "- `text` contains the full text of the review"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJZSpzfEijf_"
      },
      "source": [
        "download(\"imdb-train.csv.gz\", \"https://github.com/datascienceunibo/bbs-dl-lab-2019/raw/master/imdb-train.csv.gz\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixJ7gGxvijgD"
      },
      "source": [
        "train_set = pd.read_csv(\"imdb-train.csv.gz\", sep=\"\\t\", names=[\"label\", \"text\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u20J-GJNijgH"
      },
      "source": [
        "train_set.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GX30ypnMijgM"
      },
      "source": [
        "Let's view some rows of the dataset, after increasing the lenght of shown text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7zi6nMDijgN"
      },
      "source": [
        "pd.options.display.max_colwidth = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7tSOPO7ijgR"
      },
      "source": [
        "train_set.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBDgzSABijgX"
      },
      "source": [
        "Some HTML tags are present within reviews, specifically `<br />` to indicate newlines: we write a function which, applied on a text, replaces such tags with ASCII newline `\\n`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfNSsvFDijgY"
      },
      "source": [
        "def strip_tags(text):\n",
        "    return text.replace(\"<br />\", \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "su4ya93jijgd"
      },
      "source": [
        "We apply the function to all texts in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_QwwWL5ijge"
      },
      "source": [
        "train_set[\"text\"] = train_set[\"text\"].apply(strip_tags)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBKiXF1Tijgh"
      },
      "source": [
        "Positive and negative reviews are evenly distributed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNlJoLAjijgi"
      },
      "source": [
        "train_set[\"label\"].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckkhL-emijgl"
      },
      "source": [
        "## Multi-Layer Perceptron\n",
        "\n",
        "In their usual form, neural networks are composed by a stack of _densely connected_ layers of nodes: each node in a layer receives the output of all nodes of the underlying layer. Such networks are also known as _multi-layer perceptrons_.\n",
        "\n",
        "A MLP receives a vector as input and its topmost layer produces a vector as output, an arbitrary number of _hidden layers_ can be inserted inbetween to produce intermediate representations of data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nmAT3olijgm"
      },
      "source": [
        "Let's start by training a neural network for sentiment classification feeded with vector space model representations of reviews\n",
        "\n",
        "We initialize a vector space using tf.idf term weighting and filtering out very rare terms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXHi0YZoijgm"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vect = TfidfVectorizer(min_df=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8DNh4VIijgp"
      },
      "source": [
        "Such vector space is built upon the training reviews and their document-term matrix is produced"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WQsBrJBijgq"
      },
      "source": [
        "train_dtm = vect.fit_transform(train_set[\"text\"])\n",
        "train_dtm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBbLIk4sijgt"
      },
      "source": [
        "Similarly to training matrices for scikit-learn models, this is a 2D array where each row (1st axis) is a training observation and each column (2nd axis) is a feature: each row is a possible input to the neural network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nIpGSnPijgu"
      },
      "source": [
        "We extract the number of distinct terms in the vector space, used to define the structure of the neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75ljR5t8ijgu"
      },
      "source": [
        "num_terms = len(vect.get_feature_names_out())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTpOW3u_ijgx"
      },
      "source": [
        "num_terms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BPRJINJijgz"
      },
      "source": [
        "We want our neural network to indicate in output the correct class of each review, either \"pos\" or \"neg\"\n",
        "\n",
        "The common approach to classification with neural networks is to have one output node for each class and train them to output 1 on the right node and 0 on the others\n",
        "\n",
        "For this, we must extract from the `label` column a \"target\" matrix, where each row contains the values which the network should give as output for each review\n",
        "- `[1, 0]` for positive reviews\n",
        "- `[0, 1]` for negative reviews\n",
        "\n",
        "We define a function `make_target` which converts a given pos/neg labels series into a target matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Ag3JHLcijgz"
      },
      "source": [
        "def make_target(labels):\n",
        "    return pd.DataFrame({\n",
        "        \"pos\": labels == \"pos\", # if the label is \"pos\" then return 1 else return 0\n",
        "        \"neg\": labels == \"neg\" # if the label is \"neg\" then return 1 else 0\n",
        "    }).astype(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5u1aRO_ijg3"
      },
      "source": [
        "We then apply it to the training set labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ri-b_2sijg4"
      },
      "source": [
        "train_target = make_target(train_set[\"label\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdFtJC0eijg7"
      },
      "source": [
        "We obtain a matrix where each row is the expected network output, either `[1, 0]` (positive) or `[0, 1]` (negative)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZnzNjEQijg8"
      },
      "source": [
        "train_target.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnexXb3qijg_"
      },
      "source": [
        "Let's define the structure of the neural network\n",
        "\n",
        "We create a _sequential_ model, i.e. we define the network as a sequence of interconnected layers (alternatively, we could create non-linear structures by manually connecting layers to each other using Keras Functional API. To learn more, see: https://keras.io/guides/functional_api/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNu5VDT4ijhA"
      },
      "source": [
        "from keras.models import Sequential\n",
        "model = Sequential()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiFVkuuwyT0-"
      },
      "source": [
        "#Functional API version\n",
        "# inputs = keras.Input(shape=(num_terms,))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syikncPfijhC"
      },
      "source": [
        "In this first example we create a single-layered network, where inputs are directly connected to the output nodes\n",
        "\n",
        "As discussed above, the output nodes must be 2, one for each class; we use the _softmax_ activation function to ensure that the output is a valid probability distribution\n",
        "- we will never get a perfect `[1, 0]` as output in practice, but we will get outputs like `[0.99, 0.01]`\n",
        "\n",
        "In the first layer (in this case the only one) we also have to specify with `input_dim` the size of input vectors, in this case the number of terms in the vector space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K387d-7uijhD"
      },
      "source": [
        "from keras.layers import Dense\n",
        "model.add(Dense(2, activation=\"softmax\", input_dim=num_terms))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MprGhojHyFu8"
      },
      "source": [
        "# Functional API version\n",
        "# outputs = keras.layers.Dense(2, activation=\"softmax\", input_dim=num_terms)(inputs)\n",
        "# model = keras.Model(inputs=inputs, outputs=outputs, name=\"my_model\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiVFxjStijhF"
      },
      "source": [
        "With `summary` we can analyze the structure of our network and get the count of trainable parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaNQ0pm-ijhF"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8J83C6k9ijhI"
      },
      "source": [
        "In this case we have 35,852×2 = 71,704 weights + 2 biases = 71,706 trainable parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHMrkxAbijhI"
      },
      "source": [
        "After defining the network structure, we _compile_ it to provide some general settings of the network and initialize accordingly the underlying TensorFlow data structures"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rh-BWg-bijhJ"
      },
      "source": [
        "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nr3kQaO6ijhM"
      },
      "source": [
        "- The _optimizer_ is the algorithm used to train the network: _Adam_ and other valid options are different variants of stochastic gradient descent (SGD), including learning rate decay, momentum, etc.\n",
        "- The _loss_ is the measure to be minimized in the training process: the _cross entropy_ penalizes outputs which are not close to 1 on the correct class\n",
        "- Additional _metrics_ can be computed for evaluation purposes: we use the accuracy, i.e. the percentage of correctly classified examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KC5OK7bhijhM"
      },
      "source": [
        "We are now ready to train (_fit_) the network on given training examples, composed of inputs (tf.idf vectors) and target outputs (`[1, 0]` for positive reviews and `[0, 1]` for negative)\n",
        "\n",
        "The training examples are shuffled and used to run SGD steps on _minibatches_ of a specified size (`batch_size`): this process is repeated for a given number of training _epochs_\n",
        "\n",
        "`callbacks` parameters let us define a list of functions to be called by keras at the end of each epoch. This option is mainly used to implement some training logic (e.g. EarlyStopping) but if we runned out of memory we also could use it to call gc.collect() after each epoch in order to free some memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoX-MmG1ijhN"
      },
      "source": [
        "# Garbage Collector library\n",
        "import gc\n",
        "\n",
        "# Custom Callback To Include in Callbacks List At Training Time\n",
        "class GarbageCollectorCallback(keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        gc.collect()\n",
        "\n",
        "fit_history = model.fit(train_dtm, train_target, batch_size=200, epochs=10, callbacks=[GarbageCollectorCallback()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_rHoQ3GijhO"
      },
      "source": [
        "During the training process we see how loss and accuracy measured on the training set vary, their evolution can also be obtained from the \"history\" object returned by `fit`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZAUswkRijhP"
      },
      "source": [
        "plt.plot(fit_history.history[\"loss\"], \"ro-\")\n",
        "plt.plot(fit_history.history[\"accuracy\"], \"bo-\")\n",
        "plt.legend([\"Train loss\", \"Train accuracy\"]);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8IEIw07ijhQ"
      },
      "source": [
        "We can see in the plot how the loss progressively decreases and accuracy progressively increases through training epochs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMJIumEVijhQ"
      },
      "source": [
        "To get the raw output given by the network for a given input, we use the `predict` method: let's see for example the output for the first training review (labeled positive)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XS1kfh2gUS8l"
      },
      "source": [
        "train_dtm[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9xvYkoBijhR"
      },
      "source": [
        "model.predict(train_dtm[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0mqomIPijhS"
      },
      "source": [
        "We see that the first class (positive) has higher probability\n",
        "\n",
        "We can directly get the predicted class index with `predict_classes`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHZq9_RWijhS"
      },
      "source": [
        "# model.predict_classes(train_dtm[0].toarray()) --> [0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytFtEhPwWABp"
      },
      "source": [
        "or..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOjYjx9CU-aI"
      },
      "source": [
        "# Alternative Version working with Functional API / Tensorflow 2.0\n",
        "prediction = model.predict(train_dtm[0])\n",
        "print(prediction)\n",
        "\n",
        "np.argmax(prediction, axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4S_QpdtijhU"
      },
      "source": [
        "Let's now evaluate the network on a separate test set of labeled reviews, provided in the `imdb-test.csv.gz` file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAloDfs_ijhU"
      },
      "source": [
        "download(\"imdb-test.csv.gz\", \"https://github.com/datascienceunibo/bbs-dl-lab-2019/raw/master/imdb-test.csv.gz\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLNJREo7ijhW"
      },
      "source": [
        "test_set = pd.read_csv(\"imdb-test.csv.gz\", sep=\"\\t\", names=[\"label\", \"text\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCVKmpdHijhX"
      },
      "source": [
        "test_set.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tewY_AhijhZ"
      },
      "source": [
        "Also in this dataset we have 25,000 reviews evenly distributed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcMI31SWijha"
      },
      "source": [
        "test_set[\"label\"].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emn3o3LNijhc"
      },
      "source": [
        "As before, we apply the HTML strip function to reviews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxuwoPh7ijhc"
      },
      "source": [
        "test_set[\"text\"] = test_set[\"text\"].apply(strip_tags)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-clrrU8lijhf"
      },
      "source": [
        "We represent the test reviews in the vector space created on training reviews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ez5mF5gDijhf"
      },
      "source": [
        "test_dtm = vect.transform(test_set[\"text\"])\n",
        "test_dtm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSX4Xm2_ijhh"
      },
      "source": [
        "We then convert pos/neg labels for test examples into target vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaIS1xc4ijhi"
      },
      "source": [
        "test_target = make_target(test_set[\"label\"])\n",
        "test_target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3kfF1gxijhj"
      },
      "source": [
        "After processing the test set, we can fed it to the neural network for evaluation using the `evaluate` method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7apXO-kijhk"
      },
      "source": [
        "model.evaluate(test_dtm, test_target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Gj_OgxTijhm"
      },
      "source": [
        "The method reports the loss (first value) and the accuracy (second value) measured on the given test set: our final goal is to maximize the accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbDiUSuVijhm"
      },
      "source": [
        "Let's now introduce a _hidden layer_ in the network between input and output, for example a layer of 128 nodes with linear activation which receive input vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyvQ21f2ijhm"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(128, input_dim=num_terms))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZtBdx0p5P-t"
      },
      "source": [
        "# Functional API version\n",
        "# inputs = keras.layers.Input(shape=(num_terms,))\n",
        "# x = Dense(128)(inputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYqtpAIyijho"
      },
      "source": [
        "The output of these 128 will be fed to the output layer, composed as above by 2 nodes with softmax activation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjpSTK-3ijhp"
      },
      "source": [
        "model.add(Dense(2, activation=\"softmax\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQWstbwR5fH7"
      },
      "source": [
        "# Functional API Version\n",
        "# outputs = Dense(2, activation=\"softmax\")(x)\n",
        "# model = keras.Model(inputs = inputs, outputs=outputs, name=\"model_with_hidden_layer\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UH97JkRfijhr"
      },
      "source": [
        "The number of network parameters to be trained is now much higher"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77gkoVQOijhr"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ya32qKjyijht"
      },
      "source": [
        "Let's compile the network as before"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44A5mJmpijht"
      },
      "source": [
        "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgf2l7fqijhu"
      },
      "source": [
        "To keep compute times limited, we fit this and subsequent networks running only 3 training epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbNLJJMLijhu"
      },
      "source": [
        "model.fit(train_dtm, train_target, batch_size=200, epochs=3, callbacks=[GarbageCollectorCallback()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t452JASpijhw"
      },
      "source": [
        "Let's evaluate this new network on the same test set as before"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7ZIIDp2ijhw"
      },
      "source": [
        "model.evaluate(test_dtm, test_target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxhI79Tuijhy"
      },
      "source": [
        "Thanks to the hidden layer we had a very slight improvement, despite the lower number of training epochs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LT68VXmSijhy"
      },
      "source": [
        "To make the model more expressive, we have to introduce non-linearity in hidden layers: for example, we replicate the model above using sigmoid activation in the hidden layer\n",
        "\n",
        "We can create the model more concisely by providing the list of layers to be stacked"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xn9Rz1nNijhy"
      },
      "source": [
        "model = Sequential([\n",
        "    Dense(128, activation=\"sigmoid\", input_dim=num_terms),\n",
        "    Dense(2, activation=\"softmax\")\n",
        "])\n",
        "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4V-pU5mijh0"
      },
      "source": [
        "model.fit(train_dtm, train_target, batch_size=200, epochs=3, callbacks=[GarbageCollectorCallback()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSv2Qj2rijh2"
      },
      "source": [
        "model.evaluate(test_dtm, test_target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81z0mJXgijh3"
      },
      "source": [
        "Finally, let's test a deep model with three non-linear hidden layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeIVFiroijh3"
      },
      "source": [
        "model = Sequential([\n",
        "    Dense(256, activation=\"sigmoid\", input_dim=num_terms),\n",
        "    Dense(64, activation=\"sigmoid\"),\n",
        "    Dense(16, activation=\"sigmoid\"),\n",
        "    Dense(2, activation=\"softmax\")\n",
        "])\n",
        "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MRD8OIYijh5"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PolQv6Eijh6"
      },
      "source": [
        "model.fit(train_dtm, train_target, batch_size=200, epochs=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VU-3khHcijh8"
      },
      "source": [
        "model.evaluate(test_dtm, test_target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the right network structure has been found, you can also tune the regularization of your neural network adding Dropout layers or playing with L1/L2 reg values as in the following training example. Notice that due to the regularization effect, more training epoch could be needed."
      ],
      "metadata": {
        "id": "IDnX4xg020Hu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import regularizers\n",
        "\n",
        "model = Sequential([\n",
        "    Dense(256, activation=\"sigmoid\", input_dim=num_terms, kernel_regularizer=regularizers.L1L2(l1=1e-8, l2=1e-6)),\n",
        "    Dense(64, activation=\"sigmoid\"),\n",
        "    Dense(16, activation=\"sigmoid\"),\n",
        "    Dense(2, activation=\"softmax\")\n",
        "])\n",
        "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "1X3MCSla2WoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCe-7ay_2o9N"
      },
      "source": [
        "model.fit(train_dtm, train_target, batch_size=200, epochs=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Egtod_Qp2o9O"
      },
      "source": [
        "model.evaluate(test_dtm, test_target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phOqAYXkijh9"
      },
      "source": [
        "## Word Embedding\n",
        "\n",
        "A _word embedding_ model is a dictionary mapping each known word to a **N-dimensional vector**\n",
        "\n",
        "Such model is built by training a neural network on a bunch of text to predict the most likely word in a context defined by other words\n",
        "- training is unsupervised: no labeling of text is needed\n",
        "\n",
        "\n",
        "The resulting vector of each word somehow denotes its meaning: **semantically similar words are represented with similar vectors**. Moreover, operations between vectors can be used to **find words semantically related** to each other.\n",
        "\n",
        "Word embedding models can be used to represent text in NLP tasks, including sentiment analysis\n",
        "\n",
        "The **gensim** library provides means to represent and build word embedding models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twCE1oaJijh9"
      },
      "source": [
        "## Training a Word2Vec model\n",
        "\n",
        "We have a set of 5,000 movie reviews without any labeling: we can't train a sentiment classifier on them but we can train a word embedding model\n",
        "\n",
        "We read the compressed text file `imdb-unsup-5k.txt.gz`, containing one review per line"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OavdQnUijh-"
      },
      "source": [
        "download(\"imdb-unsup-5k.txt.gz\", \"https://github.com/datascienceunibo/bbs-dl-lab-2019/raw/master/imdb-unsup-5k.txt.gz\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyBYGNk3ijiA"
      },
      "source": [
        "with gzip.open(\"imdb-unsup-5k.txt.gz\", \"rt\", encoding=\"utf8\") as f: # open gzip file containing the dataset\n",
        "    we_train_set = [strip_tags(line.strip()) for line in f] # for each line (review) of the file, strip the string and map tags"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5GZooXWijiH"
      },
      "source": [
        "We have to preprocess each review by splitting text into tokens\n",
        "\n",
        "gensim, the library used to train the word embedding model, provides a simple utility function for this\n",
        "- alternatively any tokenization function can be used, e.g. `nltk.word_tokenize`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfutSnpbijiH"
      },
      "source": [
        "from gensim.utils import simple_preprocess"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85unLKi1ijiI"
      },
      "source": [
        "%%time\n",
        "\n",
        "we_train_tokens = [simple_preprocess(text) for text in we_train_set] # we_train_tokens will be a matrix where the first axis contains each review tokens array while the second one represents the review tokens\n",
        "\n",
        "# Wall time: time elapsed according to the computer's internal clock\n",
        "# User-cpu time: the amount of time spent executing user-code \n",
        "# Sys cpu time: the amount of time spent in the kernel due to the need of privileged operations (like IO to disk)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQK-0EreijiJ"
      },
      "source": [
        "we_train_set[0][:82]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0s69SngijiK"
      },
      "source": [
        "we_train_tokens[0][:8]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ja20QoU3ijiM"
      },
      "source": [
        "We can now use the token sequences to train the Word2Vec embedding model\n",
        "\n",
        "The most important parameter is the size of the word vectors we want to obtain\n",
        "- in the original Word2Vec paper 300 is indicated as a good value\n",
        "- here we use 50 as a tradeoff between accuracy and efficiency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TutdIlQPijiM"
      },
      "source": [
        "wordvecs_size = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RqPntI9ijiO"
      },
      "source": [
        "Other relevant parameters are\n",
        "- the _window size_, i.e. the number of words before and after any word to consider as its context\n",
        "- the minimum appearances of a term to be included in the model\n",
        "\n",
        "We specify these options in the `Word2Vec` initializer, together with the set of token sequences used to train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozAYVO4DijiO"
      },
      "source": [
        "%%time\n",
        "wv_model = gensim.models.Word2Vec(\n",
        "    we_train_tokens,\n",
        "    size=wordvecs_size,\n",
        "    window=5,\n",
        "    min_count=5\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqu4z2HdijiQ"
      },
      "source": [
        "Our Word2Vec model is now trained, we can get a reference to the word->vector mapping itself `wv` and drop the rest of the model object to free some memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJ9oaU44ijiQ"
      },
      "source": [
        "wv = wv_model.wv\n",
        "del wv_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-ZJts5QijiR"
      },
      "source": [
        "## Exploring the word embedding model\n",
        "\n",
        "How many distinct terms are represented in the model?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cqNQUweijiR"
      },
      "source": [
        "len(wv.vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mMR884oijiT"
      },
      "source": [
        "Which are these terms? `index2word` is an ordered list with more common terms coming first"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnGJ4SZpijiT"
      },
      "source": [
        "wv.index2word[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmCb-FNoijiU"
      },
      "source": [
        "Let's see the vector of a word, e.g. \"excellent\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tte_Q-IjijiU"
      },
      "source": [
        "wv.word_vec(\"excellent\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bJN2kXWijiV"
      },
      "source": [
        "We can also compute and get (L2) normalized word vectors (for each vector the sum of the respective components squares will always be up to 1), used to compute cosine similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6YbcAZtijiV"
      },
      "source": [
        "wv.init_sims()   # compute and cache normalized vectors (using L2-Normalization)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHTppXSMijiW"
      },
      "source": [
        "# True indicates to normalize the vector\n",
        "wv.word_vec(\"excellent\", True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unRqX91gijiX"
      },
      "source": [
        "The word vector by itself doesn't give much information, but we can search for example which are the words with vectors most similar to this...\n",
        "\n",
        "Let's use the `cosine_similarities` function to compute similarity between this vector and all the other ones, stored in the `vector` array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xhp5jb5ijiX"
      },
      "source": [
        "similarities_to_excellent = wv.cosine_similarities(\n",
        "    wv.word_vec(\"excellent\"),\n",
        "    wv.vectors\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eyh_jJsKijiZ"
      },
      "source": [
        "We obtain an array of cosine similarity scores that has a component for each word represented in the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oh2tZ8_hbkDA"
      },
      "source": [
        "similarities_to_excellent.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2YQMfI4ijiZ"
      },
      "source": [
        "similarities_to_excellent[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fbKEv9Pijic"
      },
      "source": [
        "Let's label them with the term they refer to and sort by descending values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yX-uIkfLijic"
      },
      "source": [
        "pd.Series(\n",
        "    similarities_to_excellent,\n",
        "    wv.index2word\n",
        ").sort_values(ascending=False).head(10) # sort the values by descending similarity score and take the first 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwyeiX8Yijih"
      },
      "source": [
        "In this way we found **other words** other than \"excellent\" with a **strong positive connotation**!\n",
        "\n",
        "For this the model provides a `most_similar` method, which also removes the reference word from the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThbC8lr_ijih"
      },
      "source": [
        "wv.most_similar(\"excellent\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eD6Rkhv-ijij"
      },
      "source": [
        "We can similarly see what happens with a strongly negative word, e.g. \"terrible\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLCZfn4Tijij"
      },
      "source": [
        "wv.most_similar(\"terrible\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRbp83_Mijil"
      },
      "source": [
        "Other strongly negative words are found!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmXN-LSoijil"
      },
      "source": [
        "Another powerful function of word embedding models is to find words with specific syntactic and semantic relationships using vector arithmetics\n",
        "\n",
        "Consider the relationship _\"man\" is to \"woman\" as \"actor\" is to X_ where the model has to find out that X = \"actress\"\n",
        "\n",
        "Word2Vec produces vectors in such a way that _\"man\" - \"woman\" = \"actor\" - X_, so we can find X as the term whose vector is closest to _\"actor\" + \"woman\" - \"man\"_\n",
        "\n",
        "Let's produce the vector representation of X..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2Qt-RL2ijim"
      },
      "source": [
        "composition = (wv.word_vec(\"actor\", True)\n",
        "             + wv.word_vec(\"woman\", True)\n",
        "             - wv.word_vec(\"man\", True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AA_gUAY0ijio"
      },
      "source": [
        "...and then find the words most similar to this composition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrKeBry3ijio"
      },
      "source": [
        "pd.Series(\n",
        "    wv.cosine_similarities(composition, wv.vectors),\n",
        "    wv.index2word\n",
        ").sort_values(ascending=False).head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQgwZw9mijiq"
      },
      "source": [
        "Also in this case we can use `most_similar`, distinguishing words with positive and negative weight"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUswR_k4ijiq"
      },
      "source": [
        "wv.most_similar(\n",
        "    positive=[\"actor\", \"woman\"],\n",
        "    negative=[\"man\"]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6AiDV0Oijir"
      },
      "source": [
        "According to randomness in the training process, the correct answer \"actress\" might be the most similar word or very close to it, but still the confidence of the model is limited"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUhtaQjaijir"
      },
      "source": [
        "We proceed our analysis on a pretrained GloVe (_Global Vectors_) word embedding model, whose training procedure is similar to Word2Vec\n",
        "\n",
        "We use a version trimmed down to the most common 100,000 terms of the 100d model trained on Wikipedia, available here: https://nlp.stanford.edu/projects/glove/\n",
        "\n",
        "Arrays with words and vectors are provided in the `glove.npz` file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtxBxAWlijir"
      },
      "source": [
        "download(\"glove.npz\", \"https://github.com/datascienceunibo/bbs-dl-lab-2019/raw/master/glove.npz\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ro3dVXHVijis"
      },
      "source": [
        "with np.load(\"glove.npz\") as f:\n",
        "    glove_words = f[\"words\"]\n",
        "    glove_vectors = f[\"vectors\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gqHrkpgijiu"
      },
      "source": [
        "We read the vector size from the loaded array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zc8Ezgocijiu"
      },
      "source": [
        "wordvecs_size = glove_vectors.shape[1]\n",
        "wordvecs_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXSAB-Fiijiv"
      },
      "source": [
        "We then create the word embedding model from the words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "810O0gzOijiv"
      },
      "source": [
        "wv = gensim.models.KeyedVectors(wordvecs_size)\n",
        "wv[glove_words.tolist()] = glove_vectors\n",
        "wv.init_sims()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXM_8bZBijiw"
      },
      "source": [
        "Searching on this model for the answer to _man : woman = actor : X_..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ts4n0Lzoijiw"
      },
      "source": [
        "wv.most_similar(\n",
        "    positive=[\"actor\", \"woman\"],\n",
        "    negative=[\"man\"]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zqk6qXhPijix"
      },
      "source": [
        "...the correct answer \"actress\" is more dominant on the other ones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHOqPlRtijix"
      },
      "source": [
        "Other examples with multiple pairs: finding the plural of a singular word..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmdt3u5Lijiy"
      },
      "source": [
        "wv.most_similar(\n",
        "    positive=[\"mouse\", \"dogs\", \"cats\"],\n",
        "    negative=[         \"dog\",  \"cat\"]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEzzTgM6ijiz"
      },
      "source": [
        "...and finding the capital of a State"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6xfzMegijiz"
      },
      "source": [
        "wv.most_similar(\n",
        "    positive=[\"france\", \"rome\",  \"berlin\"],\n",
        "    negative=[          \"italy\", \"germany\"]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtlGXDtDiji0"
      },
      "source": [
        "Another method provided by the model is `doesnt_match` finding the word which is the least related to the others in a given list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggqHsg-Ciji0"
      },
      "source": [
        "wv.doesnt_match([\"cat\", \"mouse\", \"dog\", \"keyboard\", \"frog\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbF34BR6iji1"
      },
      "source": [
        "## Representing text with word embedding\n",
        "\n",
        "We now see how to leverage the word embedding model in a neural network for sentiment classification\n",
        "\n",
        "We start by tokenizing texts of training reviews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fl71GbuEiji1"
      },
      "source": [
        "%%time\n",
        "train_tokens = [gensim.utils.simple_preprocess(text) for text in train_set[\"text\"]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxQCdAA2iji2"
      },
      "source": [
        "Let's see an example of tokenized review"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeDkVtk2iji2"
      },
      "source": [
        "train_set[\"text\"][0][:34]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExehX2fviji3"
      },
      "source": [
        "train_tokens[0][:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogIckRWKiji4"
      },
      "source": [
        "We now convert these lists of text tokens into lists of indices of terms in the word embedding model, leaving out terms not present in the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3iIvvERiji4"
      },
      "source": [
        "train_indices = [\n",
        "    [wv.vocab[word].index for word in text if word in wv.vocab] # for each token (word) in the review, that is present in our Word2Vec vocabulary, get its index in the Word2Vec vocabulary\n",
        "    for text in train_tokens # i.e. for each tokenized review\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRPPA1oeiji5"
      },
      "source": [
        "For example the begin of the review above is now represented with..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIsZ8aQniji5"
      },
      "source": [
        "train_indices[0][:5] # [0] is the index of the first review, [:5] is used to get the first five word indexes of the first review"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxbZlUmZiji6"
      },
      "source": [
        "...which translated back into words would be... (notice that the first term was removed because not in the embedding model)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OZ7A9BIiji6"
      },
      "source": [
        "[wv.index2word[i] for i in train_indices[0][:5]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHMbszeJiji8"
      },
      "source": [
        "Since we want to perform a review-level sentiment analysis, we have to find a way to represent each review using the respective word vectors.\n",
        "As a first solution, we represent each review with the mean of normalized vectors of words contained in it: we obtain such vectors for all train reviews and stack them together in a matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbiYYgapiji8"
      },
      "source": [
        "train_we_repr = np.vstack([wv.vectors_norm[indices].mean(0) for indices in train_indices]) # i.e. for each review indices, get the relative word2vec vectors, compute their means and stack the resulting vectors in a matrix.\n",
        "\n",
        "# This way we now have a matrix that has a row per training set review and as many columns as the number of word vector features (100)\n",
        "train_we_repr.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itLB8GMyiji9"
      },
      "source": [
        "We then create a MLP network with one hidden layer accepting such vectors in input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nImeccYXiji9"
      },
      "source": [
        "model = Sequential([\n",
        "    Dense(128, activation=\"sigmoid\", input_dim=wordvecs_size),\n",
        "    Dense(2, activation=\"softmax\")\n",
        "])\n",
        "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXR0j5rJiji-"
      },
      "source": [
        "As the input size of the network is much lower, so it is the number of parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7LO5JNKiji-"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zerzdpkFiji_"
      },
      "source": [
        "Training is much faster than before, so we can increment the epochs and reduce the batch size, thus making more SGD steps in each epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVyXA1WkijjA"
      },
      "source": [
        "model.fit(train_we_repr, train_target, batch_size=20, epochs=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NukXqBTwijjB"
      },
      "source": [
        "Let's preprocess test reviews as we did for training ones, thus extracting tokens and converting them to indices..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBNZ24koijjB"
      },
      "source": [
        "test_tokens = [gensim.utils.simple_preprocess(text) for text in test_set[\"text\"]]\n",
        "test_indices = [\n",
        "    [wv.vocab[word].index for word in text if word in wv.vocab]\n",
        "    for text in test_tokens\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75n6Yyz8ijjC"
      },
      "source": [
        "...and obtaining means of word vectors for each review"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_hMmd8aijjC"
      },
      "source": [
        "test_we_repr = np.vstack([wv.vectors_norm[indices].mean(0) for indices in test_indices])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPWB-1nEijjF"
      },
      "source": [
        "We can now evaluate the network on the test reviews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjiIP9UpijjF"
      },
      "source": [
        "model.evaluate(test_we_repr, test_target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sapvLiNFijjH"
      },
      "source": [
        "The accuracy is not as good as before: with this representation we lose identity of the words in the documents other than their order"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q67ydteQijjI"
      },
      "source": [
        "## Recurrent neural networks\n",
        "\n",
        "MLPs are _feed-forward_ networks: their output at any time is only dependent from their input at the same time\n",
        "\n",
        "On the other side, if we somehow introduce **memory** inside a network, we can make its output dependent from current as well as past inputs, thus we can process **sequential data**\n",
        "\n",
        "_Recurrent_ neural networks include **cyclic connections** between nodes, making the output dependent from the state of the network at previous time steps and thus from previous inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtGrQj7AijjI"
      },
      "source": [
        "### Sequential data\n",
        "\n",
        "While an input example for a MLP must be represented with a vector of size S, an example for a recurrent NN is represented with a **sequence of vectors**, fed to the network in T subsequent time steps (T is equal for all examples)\n",
        "\n",
        "Thus N input samples with input size S are no longer represented with a N×S array, but with a N×T×S array\n",
        "\n",
        "Leveraging the word embedding model, we represent each review with the **sequence of word vectors** for the terms contained in it\n",
        "- in this way, we consider both the identity of words (the vectors) and their order!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9ig7jcbijjJ"
      },
      "source": [
        "We start from the sequences of word indices `*_indices` (train_indices, test_indices) extracted above\n",
        "\n",
        "We need to make all sequences of the same length (the T term above): we set a desired sequence size T, then we trim longer sequences to that size (taking the final T elements) and pad shorter sequences with null values: Keras' `pad_sequences` function does this\n",
        "- larger T values would make training much slower"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrJ9m443ijjK"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "max_words = 200\n",
        "train_seq = pad_sequences(train_indices, max_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzShlwn7ijjL"
      },
      "source": [
        "train_seq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTgjsU67ijjN"
      },
      "source": [
        "The size of the matrix is the number of samples times the sequence length, i.e. N×T"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eso0uIZoijjO"
      },
      "source": [
        "train_seq.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9sOFUNDijjP"
      },
      "source": [
        "### Building the network\n",
        "\n",
        "Let's now create a neural network which gets such sequences as input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCVlPbB9ijjP"
      },
      "source": [
        "model = Sequential()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuPG_WpcijjQ"
      },
      "source": [
        "We first insert an `Embedding` layer, which translates each received value into the word vector from the embedding model\n",
        "\n",
        "We need to specify the size of input and output and the word vectors to be used, taking them from the model; we also specify `trainable=False` to \"freeze\" our pretrained word vectors and exclude them from training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0_fyctsijjQ"
      },
      "source": [
        "from keras.layers import Embedding\n",
        "model.add(Embedding( # [1, 2, 3, ] --> [[2.23232, 2.4423, 223,2, ....], [], [] ]\n",
        "    input_dim=len(wv.vocab),    # number of distinct vocabulary terms in Word2Vec model\n",
        "    output_dim=wordvecs_size,   # size of word vectors (S)\n",
        "    input_length=max_words,     # length of sequences (T)\n",
        "    weights=[wv.vectors],       # pretrained Word2Vec vectors\n",
        "    trainable=False\n",
        "))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-iusgIq9ijjR"
      },
      "source": [
        "The output of this layer is a N×T×S tensor, we feed it to a recurrent layer which receives S-sized vectors for T time steps\n",
        "\n",
        "_Gated Recurrent Units_ (GRU) are a simplified version of _Long Short-Term Memory_ (LSTM) units, which can potentially hold information in memory across many time steps; we use here a layer of 128 GRU cells\n",
        "\n",
        "_Dropout_ randomly drops (sets to zero) a given ratio of input values at each time step: it is a technique to prevent model overfitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dE2umoGqijjS"
      },
      "source": [
        "from keras.layers import GRU\n",
        "model.add(GRU(128, dropout=0.2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fz6cZX6tijjT"
      },
      "source": [
        "While producing 128 output values at each time step, the GRU layer by default only returns the outputs at the final steps, i.e. when the whole input sequence has been fed to the network, thus the output size of this layer is N×128 (the time dimension collapses)\n",
        "\n",
        "We can now finalize the network with the output layer, which receives the output of the GRU layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQQXSfhLijjT"
      },
      "source": [
        "model.add(Dense(2, activation=\"softmax\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQz40ZwHijjU"
      },
      "source": [
        "The model summary gives a recap of shapes of data across network layers other than parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8XBIRFIijjU"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIP-bm4SijjV"
      },
      "source": [
        "We can now compile the network and train it on the padded sequences of word indices\n",
        "- training of RNNs is quite slow, we again limit training to 3 epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpWVXKdoijjW"
      },
      "source": [
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "model.fit(train_seq, train_target, batch_size=200, epochs=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgwxQH-bijjX"
      },
      "source": [
        "Let's now obtain the padded sequences also for the test reviews..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qo6fNOX1ijjY"
      },
      "source": [
        "test_seq = pad_sequences(test_indices, max_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_seq"
      ],
      "metadata": {
        "id": "uGhdCU03Jwzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g20kvlPHijjZ"
      },
      "source": [
        "...and use them to evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oOOjw0KijjZ"
      },
      "source": [
        "model.evaluate(test_seq, test_target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3Xvt97jZAM_"
      },
      "source": [
        "We have got an higher accuracy than the previous solution, thanks to the reviews representation as word sequences and the memory capability of the GRU network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_O5CvEQVijja"
      },
      "source": [
        "## Cross domain classification\n",
        "\n",
        "We trained our network on reviews of movies and tested its ability to classify sentiment in reviews of movies\n",
        "\n",
        "Can we successfully apply our model to reviews pertaining to a different domain?\n",
        "\n",
        "The `yelp-test-10k.csv.gz` file contains 10,000 labeled user reviews about restaurants extracted from Yelp"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruAqMWKuijja"
      },
      "source": [
        "download(\"yelp-test-10k.csv.gz\", \"https://github.com/datascienceunibo/bbs-dl-lab-2019/raw/master/yelp-test-10k.csv.gz\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9WF8IjCijjb"
      },
      "source": [
        "xdom_set = pd.read_csv(\"yelp-test-10k.csv.gz\", sep=\"\\t\", names=[\"label\", \"text\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVAq0BiMijjc"
      },
      "source": [
        "xdom_set.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pH0eU8nijjf"
      },
      "source": [
        "xdom_set[\"label\"].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bn2HaiYOijji"
      },
      "source": [
        "We apply the same preprocessing steps we applied above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbvFQXrHijji"
      },
      "source": [
        "xdom_set[\"text\"] = xdom_set[\"text\"].apply(strip_tags)\n",
        "xdom_tokens = [gensim.utils.simple_preprocess(text) for text in xdom_set[\"text\"]]\n",
        "xdom_indices = [\n",
        "    [wv.vocab[word].index for word in text if word in wv.vocab]\n",
        "    for text in xdom_tokens\n",
        "]\n",
        "xdom_seq = pad_sequences(xdom_indices, max_words)\n",
        "xdom_target = make_target(xdom_set[\"label\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSNJx7E-ijjj"
      },
      "source": [
        "model.evaluate(xdom_seq, xdom_target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "co0Aap_qijjj"
      },
      "source": [
        "The network is fairly accurate, although it was trained on reviews of a different domain\n",
        "\n",
        "Can we further improve this?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m49UfOgBijjk"
      },
      "source": [
        "## Fine tuning the network\n",
        "\n",
        "In the `yelp-train-2k.csv.gz` we have a set of 2,000 labeled Yelp reviews which can be used for training\n",
        "\n",
        "We would like to make use of these in-domain reviews, without throwing away the model trained on the richer set of cross-domain reviews\n",
        "\n",
        "We can \"tune\" the trained model with an additional training run on the new set of reviews, thus making it more oriented to the new domain and still using knowledge from the other\n",
        "\n",
        "Let's load and view a summary of the file..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxlsR6mBijjk"
      },
      "source": [
        "download(\"yelp-train-2k.csv.gz\", \"https://github.com/datascienceunibo/bbs-dl-lab-2019/raw/master/yelp-train-2k.csv.gz\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxpazN57ijjk"
      },
      "source": [
        "tune_set = pd.read_csv(\"yelp-train-2k.csv.gz\", sep=\"\\t\", names=[\"label\", \"text\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCs5AdGVijjl"
      },
      "source": [
        "tune_set.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8DJfLp6ijjm"
      },
      "source": [
        "tune_set[\"label\"].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzUliT1fijjn"
      },
      "source": [
        "...and apply the usual preprocessing steps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sljc7xaKijjn"
      },
      "source": [
        "tune_set[\"text\"] = tune_set[\"text\"].apply(strip_tags)\n",
        "tune_tokens = [gensim.utils.simple_preprocess(text) for text in tune_set[\"text\"]]\n",
        "tune_indices = [\n",
        "    [wv.vocab[word].index for word in text if word in wv.vocab]\n",
        "    for text in tune_tokens\n",
        "]\n",
        "tune_seq = pad_sequences(tune_indices, max_words)\n",
        "tune_target = make_target(tune_set[\"label\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_tp7lZjijjo"
      },
      "source": [
        "We now repeat the model training process on this set of reviews: the process is very fast due to the limited size of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0UcqKYrijjo"
      },
      "source": [
        "model.fit(tune_seq, tune_target, epochs=5, batch_size=200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUtScIhQijjp"
      },
      "source": [
        "Let's now repeat the evaluation on the Yelp test set loaded before"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKPuFAkRijjp"
      },
      "source": [
        "model.evaluate(xdom_seq, xdom_target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3MGAFWQijjq"
      },
      "source": [
        "We successfully boosted the model accuracy, combining even limited knowledge of the target domain with large knowledge extracted from a different domain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKdSeUmVSXah"
      },
      "source": [
        "## Introduction to the Transformer\n",
        "The transformers library is an open-source, community-based repository to train, use and share models based on \n",
        "the Transformer architecture [(Vaswani & al., 2017)](https://arxiv.org/abs/1706.03762) such as Bert [(Devlin & al., 2018)](https://arxiv.org/abs/1810.04805),\n",
        "Roberta [(Liu & al., 2019)](https://arxiv.org/abs/1907.11692), GPT2 [(Radford & al., 2019)](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf),\n",
        "XLNet [(Yang & al., 2019)](https://arxiv.org/abs/1906.08237), etc. \n",
        "\n",
        "Along with the models, the library contains multiple variations of each of them for a large variety of \n",
        "downstream-tasks like **Named Entity Recognition (NER)**, **Sentiment Analysis**, \n",
        "**Language Modeling**, **Question Answering** and so on.\n",
        "\n",
        "### Before Transformer\n",
        "\n",
        "Back to 2017, most of the people using Neural Networks when working on Natural Language Processing were relying on \n",
        "sequential processing of the input through [Recurrent Neural Network (RNN)](https://en.wikipedia.org/wiki/Recurrent_neural_network).\n",
        "\n",
        "![rnn](http://colah.github.io/posts/2015-09-NN-Types-FP/img/RNN-general.png)   \n",
        "\n",
        "RNNs were performing well on large variety of tasks involving sequential dependency over the input sequence. \n",
        "However, this sequentially-dependent process had issues modeling very long range dependencies and \n",
        "was not well suited for the kind of hardware we're currently leveraging due to bad parallelization capabilities. \n",
        "\n",
        "Some extensions were provided by the academic community, such as Bidirectional RNN ([Schuster & Paliwal., 1997](https://www.researchgate.net/publication/3316656_Bidirectional_recurrent_neural_networks), [Graves & al., 2005](https://mediatum.ub.tum.de/doc/1290195/file.pdf)), \n",
        "which can be seen as a concatenation of two sequential process, one going forward, the other one going backward over the sequence input.\n",
        "\n",
        "![birnn](https://miro.medium.com/max/764/1*6QnPUSv_t9BY9Fv8_aLb-Q.png)\n",
        "\n",
        "\n",
        "And also, the Attention mechanism, which introduced a good improvement over \"raw\" RNNs by giving \n",
        "a learned, weighted-importance to each element in the sequence, allowing the model to focus on important elements.\n",
        "\n",
        "![attention_rnn](https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2017/08/Example-of-Attention.png)  \n",
        "\n",
        "### Then comes the Transformer  \n",
        "\n",
        "The Transformers era originally started from the work of [(Vaswani & al., 2017)](https://arxiv.org/abs/1706.03762) who\n",
        "demonstrated its superiority over [Recurrent Neural Network (RNN)](https://en.wikipedia.org/wiki/Recurrent_neural_network)\n",
        "on translation tasks but it quickly extended to almost all the tasks RNNs were State-of-the-Art at that time.\n",
        "\n",
        "One advantage of Transformer over its RNN counterpart was its non sequential attention model. Remember, the RNNs had to\n",
        "iterate over each element of the input sequence one-by-one and carry an \"updatable-state\" between each hop. With Transformer, the model is able to look at every position in the sequence, at the same time, in one operation.\n",
        "\n",
        "For a deep-dive into the Transformer architecture, [The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html#encoder-and-decoder-stacks) \n",
        "will drive you along all the details of the paper.\n",
        "\n",
        "![transformer-encoder-decoder](https://nlp.seas.harvard.edu/images/the-annotated-transformer_14_0.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BERT\n",
        "For the rest of this introduction and some summarisation tasks, we will use the [BERT (Devlin & al., 2018)](https://arxiv.org/abs/1810.04805) architecture, as it's one of the most powerful for text processing and there are plenty of content about it\n",
        "over the internet, it will be easy to dig more over this architecture if you want to. \n",
        "\n",
        "BERT was trained on a large text corpus, which gives architecture/model the ability to better understand the language and to learn variability in data patterns and generalizes well on several NLP tasks. As it is bidirectional that means BERT learns information from both the left and the right side of a token’s context during the training phase.\n",
        "\n",
        "One key point of this model is that it can be used to generate **contextual** word embeddings: as opposed to Word2Vec and GloVe each token is represented differently based on the context. For instance, in BERT the word \"bank\" is represented with two different vectors for the sentences \"open a bank account\" and \"on the river bank\" as they have different meanings. Instead W2C and GloVe are context-free representations, so they would represent \"bank\" with the same vector for all the sentences in the corpus.\n",
        "\n",
        "BERT is mostly composed by Encoder blocks from the Transformer architecture that let BERT achieve high performance in language modeling/understanding tasks.\n",
        "\n",
        "<img src=\"https://humboldt-wi.github.io/blog/img/seminar/bert/bert_architecture.png\"></img>\n",
        "\n",
        "This model is first pre-trained in two ways: \n",
        "\n",
        "1) First, it is (pre)trained on large corpora of text to solve a LM task (predicting masked words in sequences of tokens); \n",
        "<img src=\"https://jalammar.github.io/images/BERT-language-modeling-masked-lm.png\"></img>\n",
        "\n",
        "2) To make BERT better at handling relationships between multiple sentences, the pre-training process includes an additional task: Given two sentences (A and B), is B likely to be the sentence that follows A, or not?\n",
        "<img src=\"https://jalammar.github.io/images/bert-next-sentence-prediction.png\"></img>\n",
        "\n",
        "Finally BERT can be fine-tuned for downstream tasks, as sentiment classification, adding a classification (usually linear) layer on top of the model."
      ],
      "metadata": {
        "id": "RmHf31Ru63id"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFHTP6CFSXai"
      },
      "source": [
        "### Getting started with transformers\n",
        "\n",
        "\n",
        "\n",
        "The transformers library allows you to benefits from large, pretrained language models without requiring a huge and costly computational\n",
        "infrastructure. Most of the State-of-the-Art models are provided directly by their author and made available in the library \n",
        "in PyTorch and TensorFlow in a transparent and interchangeable way. \n",
        "\n",
        "You will need to install the transformers library (if not already loaded). You can do so with this command:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnT3Jn6fSXai",
        "scrolled": true
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIQGDTIDSXai"
      },
      "source": [
        "import torch\n",
        "from transformers import AutoModel, AutoTokenizer, BertTokenizer\n",
        "\n",
        "torch.set_grad_enabled(False) # we initially do not train any model so we disable gradient calculation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xMDTHQXSXai"
      },
      "source": [
        "# Store the model we want to use\n",
        "MODEL_NAME = \"bert-base-uncased\"\n",
        "\n",
        "# We need to create the model and tokenizer\n",
        "model = AutoModel.from_pretrained(MODEL_NAME)\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6EcynhYSXai"
      },
      "source": [
        "With only the above two lines of code, you're ready to use a BERT pre-trained model. \n",
        "The tokenizers will allow us to map a raw textual input to a sequence of integers representing our textual input\n",
        "in a way the model can manipulate. Since we will be using a PyTorch model, we ask the tokenizer to return to us PyTorch tensors.\n",
        "\n",
        "We can visualize this process graphically and then via code:\n",
        "<img src=\"https://jalammar.github.io/images/distilBERT/bert-distilbert-tokenization-2-token-ids.png\" />\n",
        "\n",
        "You may have noticed that the word \"rumination\" has been splitted into two tokens (rum, ##ination). This is because BERT uses what is called a WordPiece tokenizer. It works by splitting words either into the full forms (e.g., one word becomes one token) or into word pieces — where one word can be broken into multiple tokens.\n",
        "\n",
        "An example of where this can be useful is where we have multiple forms of words. For example:\n",
        "\n",
        "| Word          | Token(s)                           |\n",
        "| ------------- | ---------------------------------- |\n",
        "| surf          | \\['surf'\\]                         |\n",
        "| surfing       | \\['surf', '##ing'\\]                 |\n",
        "| surfboarding  | \\['surf', '##board', '##ing'\\]       |\n",
        "| surfboard     | \\['surf', '##board'\\]               |\n",
        "| snowboard     | \\['snow', '##board'\\]               |\n",
        "| snowboarding  | \\['snow', '##board', '##ing'\\]       |\n",
        "| snow          | \\['snow'\\]                         |\n",
        "| snowing       | \\['snow', '##ing'\\]                 |\n",
        "\n",
        "\n",
        "By splitting words into word pieces, we have already identified that the words \"surfboard\" and \"snowboard\" share meaning through the wordpiece \"##board\" We have done this without even encoding our tokens or processing them in any way through BERT.\n",
        "\n",
        "Using word pieces allows BERT to easily identify related words as they will usually share some of the same input tokens, which are then fed into the first layers of BERT."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtovcEfC3Iiv"
      },
      "source": [
        "tokens_pt = tokenizer(\"a visually stunning rumination on love\", return_tensors=\"pt\")\n",
        "\n",
        "for key, value in tokens_pt.items():\n",
        "    print(\"{}:\\n\\t{}\".format(key, value))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Og0qqOMj3Iix"
      },
      "source": [
        "The tokenizer automatically converted our input to all the inputs expected by the model. It generated some additional tensors on top of the IDs: \n",
        "\n",
        "- token_type_ids: This tensor will map every tokens to their corresponding segment (see below).\n",
        "- attention_mask: This tensor is used to \"mask\" padded values in a batch of sequence with different lengths (see below).\n",
        "\n",
        "You can check the Transformers [glossary](https://huggingface.co/transformers/glossary.html) for more information about each of those keys. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see some tokenized input examples to better understand  the meaning of the token type ids and attention masks."
      ],
      "metadata": {
        "id": "F9Pa9fWaHgGA"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pl2HIcwDSXal"
      },
      "source": [
        "# Single segment input\n",
        "single_seg_input = tokenizer(\"This is a sample input\")\n",
        "\n",
        "# Multiple segment input\n",
        "multi_seg_input = tokenizer(\"This is segment A\", \"This is segment B\")\n",
        "\n",
        "print(\"Single segment token (str): {}\".format(tokenizer.convert_ids_to_tokens(single_seg_input['input_ids'])))\n",
        "print(\"Single segment token (int): {}\".format(single_seg_input['input_ids']))\n",
        "print(\"Single segment type       : {}\".format(single_seg_input['token_type_ids']))\n",
        "\n",
        "# Segments are concatened in the input to the model, with \n",
        "print() \n",
        "print(\"Multi segment token (str): {}\".format(tokenizer.convert_ids_to_tokens(multi_seg_input['input_ids'])))\n",
        "print(\"Multi segment token (int): {}\".format(multi_seg_input['input_ids']))\n",
        "print(\"Multi segment type       : {}\".format(multi_seg_input['token_type_ids']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NtvWOgzSXam"
      },
      "source": [
        "# Padding highlight\n",
        "tokens = tokenizer(\n",
        "    [\"This is a sample\", \"This is another longer sample text\"], \n",
        "    padding=True  # First sentence will have some PADDED tokens to match second sequence length\n",
        ")\n",
        "\n",
        "for i in range(2):\n",
        "    print(\"Tokens (int)      : {}\".format(tokens['input_ids'][i]))\n",
        "    print(\"Tokens (str)      : {}\".format([tokenizer.convert_ids_to_tokens(s) for s in tokens['input_ids'][i]]))\n",
        "    print(\"Tokens (attn_mask): {}\".format(tokens['attention_mask'][i]))\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can just feed the tokenized inputs directly into our model:"
      ],
      "metadata": {
        "id": "MrJH5OoAIafD"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgkFg52fSXai"
      },
      "source": [
        "outputs = model(**tokens_pt)\n",
        "last_hidden_state = outputs.last_hidden_state\n",
        "pooler_output = outputs.pooler_output\n",
        "\n",
        "print(\"Token wise output: {}, Pooled output: {}\".format(last_hidden_state.shape, pooler_output.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBbvwNKXSXaj"
      },
      "source": [
        "As you can see, BERT outputs two tensors:\n",
        " - One with the generated representation for every token in the input `(1, NB_TOKENS, REPRESENTATION_SIZE)`\n",
        " - One with an aggregated representation for the whole input `(1, REPRESENTATION_SIZE)`\n",
        " where:\n",
        "  - `NB_TOKENS` represents the number of tokens in the sentence\n",
        "  - `REPRESENTATION_SIZE` is the dimension of the hidden layer of the chosen BERT model (BERT-base has a 768-dim hidden layer)\n",
        "\n",
        "The first, token-based, representation can be leveraged if your task requires to keep the sequence representation and you\n",
        "want to operate at a token-level. This is particularly useful for Named Entity Recognition and Question-Answering.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For example, if you want the first token ([CLS]) last hidden state:"
      ],
      "metadata": {
        "id": "Dnte7rtWD5wB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "last_hidden_state[:, 0, :].numpy().shape"
      ],
      "metadata": {
        "id": "-k3JsY3HC4rc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also visualize the tensors graphically (considering that `last_hidden_states[0]` in the image matches our `last_hidden_state` tensor)."
      ],
      "metadata": {
        "id": "35OcbAJgplPh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://jalammar.github.io/images/distilBERT/bert-output-tensor-selection.png\" />"
      ],
      "metadata": {
        "id": "HTfk6AmznvKg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The second, aggregated, representation is especially useful if you need to extract the overall context of the sequence and don't\n",
        "require a fine-grained token-level. This is the case for Sentiment-Analysis of the sequence or Information Retrieval. Note that the first token of every sequence is always a special classification token ([CLS]). The final hidden state corresponding to this token is used as the aggregate sequence representation for classification tasks. Indeed, Pooler output is calculated as the last layer hidden-state of the first token of the sequence (classification token) further processed by a Linear layer and a Tanh activation function. "
      ],
      "metadata": {
        "id": "2NohPl0dDpq7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkRYm2HESXan"
      },
      "source": [
        "### Model loading\n",
        "\n",
        "Here we load a pre-trained Bert-base (12 Encoder layers and 768-d for hidden states) model which has been fitted on cased texts.\n",
        "\n",
        "One of the most powerful feature of transformers is its ability to seamlessly move from PyTorch to Tensorflow\n",
        "without pain for the user.\n",
        "\n",
        "For the rest of this notebook we will use the PyTorch version that is the default version in Transformers library. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kubwm-wJSXan"
      },
      "source": [
        "from transformers import TFBertModel, BertModel\n",
        "\n",
        "# Let's load a BERT model for PyTorch\n",
        "model_pt = BertModel.from_pretrained('bert-base-cased', output_hidden_states=True)\n",
        "\n",
        "# Tensorflow Version\n",
        "#model_tf = TFBertModel.from_pretrained('bert-base-cased') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJ13tlzOSXan"
      },
      "source": [
        "# transformers generates a ready to use dictionary with all the required parameters for the specific framework.\n",
        "input_pt = tokenizer(\"This is a sample input\", return_tensors=\"pt\")\n",
        "\n",
        "# Tensorflow version\n",
        "# input_tf = tokenizer(\"This is a sample input\", return_tensors=\"tf\")\n",
        "\n",
        "# Let's compare the outputs\n",
        "output_pt = model_pt(**input_pt)\n",
        "\n",
        "# Tensorflow version\n",
        "# output_tf = model_tf(input_tf)\n",
        "\n",
        "# Models outputs 2 values (The value for each tokens, the pooled representation of the input sentence)\n",
        "for name in [\"last_hidden_state\", \"pooler_output\"]:\n",
        "    print(name)\n",
        "    print(output_pt[name].shape)\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Everything is great so far, but how can we get word embeddings from this? As discussed, BERT base model uses 12 layers of transformer encoders, each output per token from each layer of these can be used as a word embedding! You probably wonder, which one is the best though? \n",
        "\n",
        "Well, this depends on the task but empirically, the authors identified that one of the best performing choices was to sum the last 4 layers, which is what we will be doing.\n",
        "\n",
        "<img src=\"http://jalammar.github.io/images/bert-feature-extraction-contextualized-embeddings.png\"></img>"
      ],
      "metadata": {
        "id": "nle6TqM9siWL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As illustrated the best performing option is to concatenate the last 4 layers but in this post, the summing approach is used for convenience. More particularly, the performance difference is not that much, and also there is more flexibility for truncating the dimensions further, without losing much information."
      ],
      "metadata": {
        "id": "LDyfiEKatJZu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Last 4 hidden states can be selected by slicing the hidden_states property"
      ],
      "metadata": {
        "id": "udxwY79X1gye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_pt[\"hidden_states\"][8:12]"
      ],
      "metadata": {
        "id": "usR-PdPs0t7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One simple and really powerful Python library to deal with transformer embeddings including BERT and a large variety of NLP tasks is Flair. https://github.com/flairNLP/flair"
      ],
      "metadata": {
        "id": "JkEXll_2ROnH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQf_fpApSXao"
      },
      "source": [
        "### Want it lighter? Faster? Let's talk distillation! \n",
        "\n",
        "One of the main concerns when using these Transformer based models is the computational power they require. All over this notebook we are using BERT model as it can be run on common machines but that's not the case for all of the models.\n",
        "\n",
        "For example, Google released **T5** an Encoder/Decoder architecture based on Transformer and available in `transformers` with no more than 11 billions parameters. Microsoft also recently entered the game with **Turing-NLG** using 17 billions parameters. This kind of model requires tens of gigabytes to store the weights and a tremendous compute infrastructure to run such models which makes it impracticable for the common man !\n",
        "\n",
        "![transformers-parameters](https://raw.githubusercontent.com/huggingface/notebooks/main/examples/images/model_parameters.png)\n",
        "\n",
        "With the goal of making Transformer-based NLP accessible to everyone Huggingface developed models that take advantage of a training process called **Distillation** which allows us to drastically reduce the resources needed to run such models with almost zero drop in performances.\n",
        "\n",
        "Intuitively you can think of distillation as a process in which a lighter model is trained to replicate the predictions made by another larger model.\n",
        "\n",
        "Going over the whole Distillation process is out of the scope of this notebook, but if you want more information on the subject you may refer to [this Medium article written by my colleague Victor SANH, author of DistilBERT paper](https://medium.com/huggingface/distilbert-8cf3380435b5), you might also want to directly have a look at the paper [(Sanh & al., 2019)](https://arxiv.org/abs/1910.01108)\n",
        "\n",
        "In `transformers` some models have been distilled and made available directly in the library. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfxMOXb-SXao"
      },
      "source": [
        "from transformers import DistilBertModel\n",
        "\n",
        "bert_distil = DistilBertModel.from_pretrained('distilbert-base-cased')\n",
        "input_pt = tokenizer(\n",
        "    'This is a sample input to demonstrate performance of distiled models especially inference time', \n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "# Forward pass time comparison between BERT and DistillBERT\n",
        "%time _ = bert_distil(input_pt['input_ids'])\n",
        "%time _ = model_pt(input_pt['input_ids'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentiment Classification with DistillBERT\n",
        "\n"
      ],
      "metadata": {
        "id": "k5oWmY0MblUK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we replicate the experiment above about in-domain sentiment classification of the IMDB dataset, but this time using a BERT-based model via the HuggingFaces Transformers library. This will give you some insights how BERT can be used to improve the results in this task. "
      ],
      "metadata": {
        "id": "QCgxUDNfX3vx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's convert the labels to integer values using sklearn LabelEncoder class."
      ],
      "metadata": {
        "id": "-3uG1NnTJh61"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "# String labels conversion to integers\n",
        "le = preprocessing.LabelEncoder()\n",
        "\n",
        "train_labels = le.fit_transform(train_set.label)\n",
        "train_labels"
      ],
      "metadata": {
        "id": "p99KaJo-Jpzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# String labels conversion to integers\n",
        "test_labels = le.fit_transform(test_set[\"label\"])\n",
        "test_labels"
      ],
      "metadata": {
        "id": "ECsdi1I-J-dt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MA-d5SSY5zlN"
      },
      "source": [
        "We already have a train and test dataset, but let's also also create a validation set which we can use for for evaluation\n",
        "and tuning without training our test set results. Sklearn has a convenient utility for creating such splits:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQoo2V4U5zlO"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(train_set[\"text\"].tolist(), train_labels, test_size=.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVfQ6hVv5zlO"
      },
      "source": [
        "Alright, we are read in our dataset. Now let's tackle tokenization. We'll eventually train a classifier using\n",
        "pre-trained DistilBert, so let's use the DistilBert tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_SLf3G75zlP"
      },
      "source": [
        "from transformers import DistilBertTokenizerFast\n",
        "\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vj2760j85zlP"
      },
      "source": [
        "Now we can simply pass our texts to the tokenizer. We'll pass `truncation=True` and `padding=True`, which will\n",
        "ensure that all of our sequences are padded to the same length and are truncated to be no longer model's maximum input\n",
        "length. This will allow us to feed batches of sequences into the model at the same time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcLf_BZm5zlQ"
      },
      "source": [
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
        "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
        "test_encodings = tokenizer(test_set[\"text\"].tolist(), truncation=True, padding=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-ysjUdG5zlQ"
      },
      "source": [
        "Now, let's turn our labels and encodings into a Dataset object. In PyTorch, this is done by subclassing a\n",
        "`torch.utils.data.Dataset` object and implementing `__len__` and `__getitem__`. In TensorFlow, we pass our input\n",
        "encodings and labels to the `from_tensor_slices` constructor method. We put the data in this format so that the data\n",
        "can be easily batched such that each key in the batch encoding corresponds to a named parameter of the\n",
        "`DistilBertForSequenceClassification.forward` method of the model we will train."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5595ba05zlR"
      },
      "source": [
        "## PYTORCH CODE\n",
        "import torch\n",
        "\n",
        "class IMDbDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = IMDbDataset(train_encodings, train_labels)\n",
        "val_dataset = IMDbDataset(val_encodings, val_labels)\n",
        "test_dataset = IMDbDataset(test_encodings, test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e06toXF35zlS"
      },
      "source": [
        "Now that our datasets our ready, we can fine-tune a model either with the 🤗\n",
        "`Trainer`/`TFTrainer` or with native PyTorch/TensorFlow. See [training](https://huggingface.co/transformers/training.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSaVZYo45zlT"
      },
      "source": [
        "#### Fine-tuning with Trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FhdEJmo89Mr"
      },
      "source": [
        "Let's create our custom metrics calculation function in order to measure the performance of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEv-E07ATWRg"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTXkX7NW5zlT"
      },
      "source": [
        "The steps above prepared the datasets in the way that the trainer is expected. Now all we need to do is create a model\n",
        "to fine-tune, define the `TrainingArguments`/`TFTrainingArguments` and\n",
        "instantiate a `Trainer`/`TFTrainer`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hV_7jKrOqjG"
      },
      "source": [
        "from transformers import DistilBertForSequenceClassification\n",
        "\n",
        "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's perform the training of our BERT model. We limit the fine-tuning to 1 epoch for time reasons. To get a better performance here you should fine-tune at least for 2-4 epochs."
      ],
      "metadata": {
        "id": "Ak2jRd5wLk1e"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPk_bOh75zlU"
      },
      "source": [
        "## PYTORCH CODE\n",
        "from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments, BertForSequenceClassification\n",
        "\n",
        "torch.set_grad_enabled(True) # Enable gradient calculation to perform the training\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    num_train_epochs=1,              # total number of training epochs\n",
        "    per_device_train_batch_size=16,  # batch size per device during training\n",
        "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        "    logging_steps=10,\n",
        "    do_eval=True,                    # enable/disable the evaluation on the validation set during the training\n",
        "    evaluation_strategy='steps',     # whether to validate the model each N steps or at the end of each epoch\n",
        "    eval_steps=200                   # If evaluation strategy is set to 'steps' then the model will be evaluted each 'eval_steps' on the validation set during the training\n",
        ")\n",
        "# If you want to customize more the training arguments...\n",
        "# https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=val_dataset,            # evaluation dataset\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49MrvWlkpMis"
      },
      "source": [
        "Once the model is trained we can evaluate it on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKXcpaR6PPFB"
      },
      "source": [
        "trainer.evaluate(test_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We successfully boosted the performance got by the RNNs previously, even if we used the distilled version of BERT fine-tuned for only one epoch. BERT larger models can reach accuracies up to almost 96% on this dataset and task (http://nlpprogress.com/english/sentiment_analysis.html)."
      ],
      "metadata": {
        "id": "rQyJc2G3hJh1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, you can try to perform the sentiment classification on a arbitrary sentence."
      ],
      "metadata": {
        "id": "7kYGLLACdtvM"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMOec1eCAl-m"
      },
      "source": [
        "sentence_to_be_classified = \"I won't buy again this product.\"  # You can type an arbitrary sentence to be summarized\n",
        "\n",
        "on_demand_test_encodings = tokenizer([sentence_to_be_classified], truncation=True, padding=True)\n",
        "on_demand_test_dataset = IMDbDataset(on_demand_test_encodings, le.transform(['neg']))\n",
        "\n",
        "result = trainer.predict(on_demand_test_dataset)\n",
        "print(\"Logits: \" + str(result.predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We got the logit predictions, but if you want probabilities you have to convert them using a softmax transformation. Then we can get the predicted class."
      ],
      "metadata": {
        "id": "_DeYBaU-MMqW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Convert logits to probabilities using softmax\n",
        "p = torch.nn.functional.softmax(torch.from_numpy(result.predictions), dim=1)\n",
        "print(\"Probabilities: \" + str(p))\n",
        "\n",
        "# Get the predicted classes for each output\n",
        "top_p, top_class = p.topk(1, dim = 1)\n",
        "print(\"Top class: \" + str(top_class[0][0].item()))\n",
        "\n",
        "print() \n",
        "\n",
        "if (top_class.numpy()[0] == 1):\n",
        "  print('The sentence polarity is positive')\n",
        "else:\n",
        "  print('The sentence polarity is negative.')"
      ],
      "metadata": {
        "id": "d8TAJoL1MIgU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}