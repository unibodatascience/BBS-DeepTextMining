{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "project-work-group-6.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0aegAFNihDz"
      },
      "source": [
        "# Text Mining Project Work (Group 6)\n",
        "\n",
        "**Text Classification and Sentiment Analysis**\n",
        "\n",
        "_Prof. Gianluca Moro, Dott. Ing. Nicola Piscaglia – DISI, University of Bologna_\n",
        "\n",
        "**Bologna Business School** - Alma Mater Studiorum Università di Bologna"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOBjMeN9i6TV"
      },
      "source": [
        "## Instructions\n",
        "- The provided exercises must be executed by the students of Group 6\n",
        "- At the end, the file must contain all the required results (as code cell outputs) along with all the commands necessary to reproduce them; \n",
        "- The function of every command or group of related commands\n",
        "must be documented clearly and concisely. \n",
        "- The submission deadline is the 1st July 2022.\n",
        "- When finished, one team member will send the notebook file (having .ipynb extension) via mail (using your BBS email account) to the teacher (nicola.piscaglia@bbs.unibo.it) indicating “[BBS Teamwork] Your last names” as subject, also keeping an own copy of the file for safety.\n",
        "- You are allowed to consult the teaching material and to search the Web for quick reference. \n",
        "- If still in doubt about anything, ask the teacher\n",
        "- It is severely NOT allowed to communicate with other teams. Ask the teacher for any clarification about the exercises.\n",
        "- Each correctly developed point counts 2/30"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "U51JJBSVeIvz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the following to import some necessary packages and download all the needed files."
      ],
      "metadata": {
        "id": "wq-rJLYrfGho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from urllib.request import urlretrieve\n",
        "import pandas as pd\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "metadata": {
        "id": "PnCN7H1geyyf"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download(file, url):\n",
        "    if not os.path.exists(file):\n",
        "        urlretrieve(url, file)"
      ],
      "metadata": {
        "id": "fu_7tBMOepHM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "download(\"reviews-electronics.csv.gz\", \"https://www.dropbox.com/s/5aidj1ns3wiuchi/reviews-electronics.csv.gz?dl=1\")\n",
        "download(\"reviews-home.csv.gz\", \"https://www.dropbox.com/s/9dlvc0nntibibk3/reviews-home.csv.gz?dl=1\")\n",
        "download(\"reviews-books.csv.gz\", \"https://www.dropbox.com/s/otbdd2u7x9ylzku/reviews-books.csv.gz?dl=1\")"
      ],
      "metadata": {
        "id": "urxyu9RweWo8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "download(\"positive-words.txt\", \"https://www.dropbox.com/s/pmju477pv8ayzho/positive-words.txt?dl=1\")\n",
        "download(\"negative-words.txt\", \"https://www.dropbox.com/s/yy4l1ezlrsar8cf/negative-words.txt?dl=1\")"
      ],
      "metadata": {
        "id": "_PZbKB6qfM0G"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Exercises"
      ],
      "metadata": {
        "id": "arGpnMs2eKMB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Load the dataset contained in `reviews-electronics.csv.gz` file in a new dataframe named `reviews_A`. Then load the dataset contained in `reviews-home.csv.gz` file in a new dataframe named `reviews_B` and the dataset contained in `reviews-books.csv.gz` in a new dataframe named `reviews_C`. Finally, read from positive-words.txt and negative-words.txt files the opinion word lists putting them to two new variables `pos_words` and `neg_words` respectively"
      ],
      "metadata": {
        "id": "lBSt1y2agFE9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJADgDZFihEG"
      },
      "source": [
        "2) Print the first five rows of the two datasets. Then, print the cardinality of the 3 `reviews_X` datasets and the distribution of the `label` feature."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXg0Vw1kihEX"
      },
      "source": [
        "3) Split `reviews_A` into train and test set by selecting the first reviews half as train set and the second one as test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c357B6OmihEY"
      },
      "source": [
        "4) Classify the reviews provided in the `reviews_A` test set by first assigning to each a score equal to the sum of scores of known words within it and then, return `\"pos\"` for reviews with a positive score and `\"neg\"` for reviews with a negative or null score. \n",
        "\n",
        "Score each word:\n",
        " - -1 if it is found in negative words list\n",
        " - -2 if it is found in negative word list and it is preceded by the word \"very\"\n",
        " - +1 if it is found in positive words list\n",
        " - +2 if it is found in positive word list and it is preceded by the word \"very\"\n",
        "\n",
        "\n",
        "Start with the setup of NLTK and the definition of the scoring function.\n",
        "Then, apply the function to all the `reviews_A` in the test set.\n",
        "Finally, compare the obtained labels with the known ones and compute the accuracy as the ratio of matches."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyroDWVoihEz"
      },
      "source": [
        "5) Create a pipeline including a `CountVectorizer` to convert reviews into word count vectors (excluding words that appear in less than 3 documents) and a `LogisticRegression` model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qy6cAiuihE5"
      },
      "source": [
        "6) Train the model on all `reviews_B` data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdC3yKxxihFC"
      },
      "source": [
        "7) Evaluate the model on the `reviews_A` test set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Crs7chcBihFP"
      },
      "source": [
        "8) Create a new pipeline as above, but replacing the `CountVectorizer` in the pipeline with a `TfidfVectorizer` and the `LogisticRegression` model with a `MultinomialNB` one."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9) Fit the new pipeline all the `reviews_B` data and evaluate the new model on the `reviews_A` test set"
      ],
      "metadata": {
        "id": "nd320SxXu6_9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SCmzFpYihFm"
      },
      "source": [
        "10) Repeat points 8 and 9 but set the `ngram_range` parameter of the `TfidfVectorizer` to use only bigrams."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Z8-7qwTihF4"
      },
      "source": [
        "11) Repeat the evaluation of the three models above, this time on the all the `reviews_C` data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "12) Tokenize the `reviews_A` train reviews and use them to build a 300-dimensional Word2Vec vector space using a window size equals to 5 and excluding all the terms that appear less than 7 times."
      ],
      "metadata": {
        "id": "r_OB8HmTMN7F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13) Convert the tokenized training reviews into a list of lists of terms indices in the Word2Vec model, leaving out terms not present in the model.\n",
        "\n"
      ],
      "metadata": {
        "id": "Wp6hj-G5zPAd"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkEokwmktSlW"
      },
      "source": [
        "14) Make all indices sequences of the same length (250 words for each review), trimming longer sequences to that size and padding shorter sequences with zero values."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "15) Train a LSTM or GRU neural network of your choice on the training sequences defined above. \n",
        "\n",
        "Finally, test the neural network on `reviews_A` test reviews. Try to maximize the accuracy on test data. "
      ],
      "metadata": {
        "id": "M-tIxApgv00-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16) Extra: train/fine-tune a transformer-based model (e.g. BERT) on `reviews_A` training reviews and evaluate it on the `reviews_A` test reviews."
      ],
      "metadata": {
        "id": "8JSVjRY1gDzj"
      }
    }
  ]
}